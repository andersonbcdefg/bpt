# target size is ~500M parameters. don't decrease hidden size for GLU.
vocab_size: 50304 # 50432 didn't work
seq_len: 512
d_model: 2560
d_ffn: 7168
d_qkv: 128
n_heads: 20
n_layers: 32
dropout: 0.0
tie_weights: True # might not play nice with torch.compile but who cares
rotary_emb: True
rotary_pct: 0.25
use_xpos: True
xpos_scale_base: 512
stable_embedding: False

  