# target size is ~500M parameters. don't decrease hidden size for GLU.
vocab_size: 50304 # 50432 didn't work
seq_len: 512
d_model: 2048
d_ffn: 5120
d_qkv: 128
n_heads: 16
n_layers: 24
dropout: 0.0
tie_weights: True # might not play nice with torch.compile but who cares
fused_transformer_block: True
rotary_emb: True
rotary_pct: 0.25
use_xpos: True
xpos_scale_base: 512
stable_embedding: False

  